import os
from dotenv import load_dotenv
from langchain_community.chat_models import ChatOpenAI
from langchain_community.llms.ollama import Ollama  
from langchain_core.output_parsers import StrOutputParser
from langchain.document_loaders import PyPDFLoader  
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import DocArrayInMemorySearch
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_community.embeddings import OllamaEmbeddings
from pydantic import ValidationError


def load_env_variables():
    """Load environment variables from .env file."""
    load_dotenv()
    return os.getenv("OPENAI_KEY")

def setup_model_and_embeddings(openai_key, model_name):
    """
    Setup the chat model and embeddings based on the selected model.

    Args:
        openai_key (str): The OpenAI API key.
        model_name (str): The model name to use (e.g., GPT, Llama).

    Returns:
        chat: The initialized chat model.
        embeddings: The embeddings object to use for document search.
    """
    if model_name.startswith("GPT"):
        chat = ChatOpenAI(api_key=openai_key, model=model_name)
        embeddings = OpenAIEmbeddings()
    else:
        chat = Ollama(model=model_name)
        embeddings = OllamaEmbeddings()
    
    return chat, embeddings

def load_and_split_pdf(pdf_path):
    """
    Load and split a PDF into pages.

    Args:
        pdf_path (str): Path to the PDF file.

    Returns:
        pages: List of pages from the PDF document.
    """
    loader = PyPDFLoader(pdf_path)
    pages = loader.load_and_split()
    return pages

def create_prompt_template():
    """
    Create a prompt template for answering questions based on context.

    Returns:
        prompt: The initialized prompt template.
    """
    template = """
    Answer the question based on the context below, If you can't
    answer the question reply I don't know. 

    context: {context}

    Question: {Question}
    """
    return PromptTemplate.from_template(template)

def generate_response(chat, parser, prompt, context, question):
    """
    Generate a response by invoking the chat model with the prompt.

    Args:
        chat: The chat model.
        parser: The output parser.
        prompt: The prompt template.
        context (str): The context for the question.
        question (str): The question to be answered.

    Returns:
        response: The response generated by the chat model.
    """
    chain = prompt | chat | parser
    response = chain.invoke({
        "context": context,
        "Question": question
    })
    return response

def create_vector_store(pages, embeddings):
    """
    Create a vector store from the loaded PDF pages and embeddings.

    Args:
        pages: List of document pages.
        embeddings: The embeddings object for vector search.

    Returns:
        vectorstore: The initialized vector store.
    """
    return DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)

def query_vector_store(vectorstore, query):
    """
    Query the vector store using a specific query string.

    Args:
        vectorstore: The vector store to query.
        query (str): The query string.

    Returns:
        result: The result of the query from the vector store.
    """
    retriever = vectorstore.as_retriever()
    return retriever.invoke(query)


# Main function to orchestrate all tasks
def main():
    # Step 1: Load environment variables
    openai_key = load_env_variables()

    # Step 2: Set up the chat model and embeddings
    MODEL = "llama2"
    chat, embeddings = setup_model_and_embeddings(openai_key, MODEL)

    # Step 3: Load and split the PDF
    pdf_path = "mulu_training.pdf"
    pages = load_and_split_pdf(pdf_path)

    # Step 4: Create the prompt template
    prompt = create_prompt_template()

    # Step 5: Generate a response using context and question
    context = "The name I was given is Harshita"
    question = "what is my name?"
    parser = StrOutputParser()
    response = generate_response(chat, parser, prompt, context, question)
    print("Chat response:", response)

    # Step 6: Create a vector store from the PDF pages
    vectorstore = create_vector_store(pages, embeddings)

    # Step 7: Query the vector store
    query = "store policy"
    vector_response = query_vector_store(vectorstore, query)
    print("Vector store response:", vector_response)

if __name__ == "__main__":
    main()
